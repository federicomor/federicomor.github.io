<!--This file was generated, do not modify it.-->
Partiamo caricando tutti gli strumentopoli misteriosi necessari alla soluzione.

````julia:ex1
using Random, LinearAlgebra
using SparseArrays
using DataStructures
using IterativeSolvers
using Plots, GraphRecipes
Random.seed!(31032025);
````

Per iniziare ad ambientarci nel contesto del problema (ovvero con il meccanismo di spostamento delle palline nelle ciotole, la condizione di terminazione del gioco, ecc) possiamo implementare una semplice simulazione del gioco stesso, osservando quali possibili scenari possono verificarsi.

````julia:ex2
# n bowls, m balls
function simulate_F(n::Int, m::Int; num_trials::Int=10000, verbose=false)
    total_moves = 0
	if verbose num_trials = 1 end

    for _ in 1:num_trials
        # random initial distribution of balls
        balls = rand(1:n, m) # array of size m, each element in 1:n
        # so that each ball i is assigned to the bowl given by balls[i]
        moves = 0

        while length(unique(balls)) > 1 # stop when all balls are in the same bowl
        # and they will be in the same bowl when all the indexes of balls vector will be equal
            ball_idx = rand(1:m) # pick a ball at random
            direction = rand([-1, 1]) # pick a direction (-1 for counterclockwise, 1 for clockwise)
			if verbose println("Balls are in bowls: $balls (selected ball: $ball_idx, will move by: $direction)") end
            balls[ball_idx] = mod1(balls[ball_idx] + direction, n) # move the ball
            # this changes the bowl attribution of ball ball_idx, using mod1 as for the circular structure
            moves += 1
        end
		total_moves += moves
		if verbose
			println("Balls are in bowls: $balls")
			println("Total moves: $total_moves")
		end
    end
    return total_moves / num_trials  # estimate of F(n, m)
end
````

Con cui possiamo o simulare il problema per ottenere una stima più o meno accurata:

````julia:ex3
simulate_F(2, 3, num_trials = 20_000) # soluzione esatta: 9/4 = 2.25
````

o altrimenti vedere in concreto cosa accade in un'esempio di esecuzione:

````julia:ex4
simulate_F(2, 3, verbose=true)
````

Ma ovviamente questo non ci basta: vogliamo risolvere il problema in modo esatto. Per farlo, l'idea è di affidarci alle _catene di Markov_. Le catene di Markov hanno infatti come principale scopo nella vita quello di poter modellare un contesto stocastico e dinamico, che è precisamente quello che abbiamo qui: palline disposte inizialmente in modo casuale (da cui lo "stocastico"), che si muovono (da cui il "dinamico") nelle ciotole sempre seguendo mosse casuali.

Una catena di Markov è caratterizzata da due elementi: lo spazio degli stati $S$, ovvero le configurazioni che può assumere il sistema che stiamo modellando, e la matrice di transizione $P$, le cui componenti $p_{ij}$ descrivono la probabilità che da un certo stato $i$ ci spostiamo in un certo stato $j$. Una volta definiti questi due elementi, la variabile $X_n$ descriverà in quale  stato $s\in S$ ci troveremo all'istante di tempo $n\in\mathbb{N}_0$. Per esempio $X_0$ è lo stato da cui partiamo, poi al tempo 1 ci sposteremo in un qualche stato $X_1$, poi $X_2$, e così via.

### Stati
In questo problema, una scelta interessante consiste già nel capire come modellare gli stati. Esistono infatti diversi possibili approcci:
1. assegnare a ogni pallina un'etichetta corrispondente alla ciotola in cui è contenuta
2. assegnare ad ogni ciotola il numero di palline che contiene, pensando alle ciotole come disposte in una fila (quindi esiste la "prima" ciotola, la "seconda" ciotola, ecc) ma mantenendo ovviamente la configurazione originale del cerchio permettendo che dalla ciotola uno possiamo "andare a sinistra" ed entrare nell'ultima ciotola, e viceversa
3. assegnare ad ogni ciotola il numero di palline che contiene, pensando alle ciotole come effettivamente disposte nel cerchio che prevede il testo, quindi accorpando in base alle simmetrie circolari le configurazioni derivate dal metodo del punto 2

Consideriamo un piccolo esempio con lo stato in cui ci sono 4 ciotole e 2 palline, disposte in modo alternato nelle ciotole. Col primo metodo, questa configurazione sarebbe descritto da quattro possibili stati: $13$, $24$, $31$, $42$. Col secondo metodo, avremmo $0101$ oppure $1010$, mentre solo col terzo metodo possiamo apprezzare la simmetria del problema e avere un unico stato, mettiamo $0101$, che rappresenti univocamente la situazione.

Avendo $n$ ciotole ed $m$ palline, ogni caso prevederebbe una diversa numerosità dello spazio degli stati:
1. con la prima formulazione, avremmo che ogni pallina può andare in ciascuna ciotola. Quindi: $n$ opzioni per ciascuna delle $m$ palline, da cui $n^m$ stati
2. con la seconda formulazione, avremmo che ogni ciotola, disposta in fila, può accogliere da zero a $m$ palline. Pensando di rappresentare ogni pallina come, beh, una pallina "∘", ed ogni ciotola come una barra "|", segue che gli stati totali sono i modi in cui possiamo anagrammare la stringa formata da $m$ palline e $n-1$ ciotole (meno uno perché fissiamo una certa ciotola essere la prima lettera della stringa), supponendo che le palline alla destra di una barra corrispondano a quelle che tale ciotola contiene[^1]. Per esempio, la stringa "∘∘||∘" corrisponde ad uno scenario con 3 palline e 3 ciotole, dove nella prima ci stanno due palline, nella seconda zero, e nella terza una. Quindi, in questo caso, il numero di stati è pari a $$\frac{(m+n-1)!}{m!(n-1)!}={ m+n-1 \choose m}$$
3. con la terza formulazione, più complicata perché contiamo come una sola configurazione, per via delle simmetrie, configurazioni precedentemente diverse, il numero degli stati diminuisce ulteriormente. Per sapere di quanto diminuisce... beh, di certo esisterà una formula che, come nei casi precedenti, conti elegantemente gli stati in funzione di $n$ ed $m$. Tuttavia ricavarla è molto meno immediato, quindi non ci ho badato troppo: la mia natura ingegneristica ha preso il sopravvento e ho beatamente proseguito col resto del problema senza soffermarmi troppo su questo calcolo

Il punto è infatti che all'aumentare del numero di ciotole e palline, il terzo metodo permette di limitare la numerosità degli stati, mantenendo quindi il problema più agilmente risolvibile per un computer. Per esempio, con 12 ciotole e 6 palline, il primo metodo prevederebbe $12^6=2985984$ stati, il secondo ${17 \choose 6}=12376$, il terzo 561. E siccome in seguito vedremo che ci sarà da risolvere un sistema, meglio lavorare con sistemi da 561 variabili rispetto che 2985984![^2]

[^1]: Per la cronaca, è il "classico" problema delle stelline e sbarrette.
[^2]: Se avete letto il "!" come fattoriale tranquilli, non siete i soli, ormai quando anche nelle storie di instagram vedo scritto _"buon compleanno! +20!"_ lo leggo come se una persona stesse compiendo "20 fattoriale" anni.

### Probabilità iniziali
Una volta definito come modellare gli stati, il prossimo passo consiste nel calcolare le probabilità che la configurazione iniziale delle palline nelle ciotole sia esattamente un certo stato piuttosto che un altro. Questo servirà perché per trovare il tempo medio necessario ad entrare nella classe ricorrente $R$, quella in cui tutte le palline sono riunite nella stessa ciotola e quindi il gioco termina, dovremmo espandere[^3] il calcolo su tutti i possibili stati da cui possiamo partire. Ovvero, moralmente dobbiamo pesare i contributi portati da ciascuno stato per la probabilità di partire in effetti da quello stato:
$$ \mathbb{E}(\text{#passi per entrare in $R$}) =  \sum_{i \in S} \mathbb{E}(\text{#passi per entrare in $R$ partendo da $i$}) \cdot \mathbb{P}(\text{partiamo da $i$})$$

Intuitivamente ha senso: se da uno stato magari ci mettiamo pochi passi per entrare nella classe ricorrente $R$, ma quella configurazione iniziale è molto rara da ottenere, allora è giusto che il suo contributo venga scalato rispetto agli altri.

Comunque, tornando al discorso, per calcolare queste probabilità serve invocare la distribuzione Multinomiale, dove diciamo che ${\bf{X}}=(X_1,\ldots,X_n) \sim \text{Mult}(m,(p_1,\ldots,p_n))$ se ${\bf{X}}$ modella lo scenario in cui, ripetendo un certo esperimento aleatorio per $m$ volte, sono risultate $X_i$ occorrenze di ciascun evento $i$, il quale aveva probabilità $p_i$ di verificarsi[^4].

Questa fa precisamente al caso nostro: come $m$ esperimenti aleatori abbiamo le assegnazioni di ciascuna delle $m$ palline a una qualunque delle $n$ ciotole scelta in modo casuale e con equiprobabilità. Quindi il vettore $\bf{p}$ delle probabilità sarà un vettore lungo $n$ e di componenti tutte uguali a $\frac{1}{n}$. La probabilità di ottenere una certa configurazione $\bf{X}=\bf{x}$ sarà quindi data dalla densità Multinomiale: $$p_{\bf{X}}({\bf x}) = \mathbb{P}({\bf X}={\bf x}) = \frac{m!}{x_1!\cdots x_n!}\prod_{i=1}^n p_i^{x_i}$$

Questa probabilità va però poi aggiustata contando per le simmetrie. Nel nostro caso, infatti, uno stato del tipo $00k0$ sarebbe equivalente a $k000$, $0k00$, e $000k$, dato che sono tutti simmetrici rispetto alla disposizione circolare delle ciotole. L'aggiustamento consiste quindi nel moltiplicare quella probabilità "base" per il numero di occorrenze che tale configurazione presenta.
Per esempio, con $n=4$ ciotole e $m=2$ palline, consideriamo lo stato $0002$. Questo può manifestarsi in 4 possibili casi (come appena spiegato, sciogliendo la simmetria circolare), quindi la probabilità di ottenere tala configurazione è pari a $$p(0002) = 4\cdot\frac{2!}{0!0!0!2!} \left(\frac{1}{4}\right)^0\left(\frac{1}{4}\right)^0\left(\frac{1}{4}\right)^0 \left(\frac{1}{4}\right)^2 = \frac{1}{4}$$

[^3]: Tecnicamente, si può dire anche esplodere o disintegrare! molto più divertenti.
[^4]: Come vincoli, abbiamo da chiedere che $\sum p_i=1$ (qualcosa deve sempre verificarsi) e $\sum X_i = m$ (le occorrenze di ciascun evento ammontano al totale di esperimenti condotti).

### Finalmente un po' di codice
Le funzioni qui di seguito si occupano quindi di generare gli stati per la catena di Markov, di cui ne propongo alcuni esempietti di esecuzione con l'opzione `verbose` attiva, in modo da facilitarne la comprensione.

````julia:ex5
# generate all possible states of where balls can be located in the bowls
function get_all_states(m::Int, n::Int; verbose=false)
	if m == 1
		return [[n]]
	end
	ret = []
	for i in 0:n
		if verbose @show i end
		tmp = get_all_states(m - 1, n - i, verbose=false)
		for t in tmp
			if verbose println("\tt=$t") end
			push!(t, i)
			push!(ret, t)
		end
	end
	return ret
end
get_all_states(3,2,verbose=true)
````

````julia:ex6
# reduce states to their minimal representation, ie accounting for simmetries
function min_state(best::Vector; verbose=false)
    tmp = copy(best)
    rtmp = reverse(copy(best))
	if verbose @show tmp,rtmp,best end
    for i in 1:length(best)
        tmp = circshift(tmp, 1)
        if tmp < best
            best = copy(tmp)
        end
        rtmp = circshift(rtmp, 1)
        if rtmp < best
            best = copy(rtmp)
        end
		if verbose @show tmp,rtmp,best end
    end
    return best
end
min_state([1,2,0,0,1],verbose=true)'
````

````julia:ex7
# compute initial probabilities for the states
function get_states(bowls::Int, balls::Int; verbose=false)
	allStates = get_all_states(bowls, balls)
	multiStates = OrderedDict{Vector{Real},Int}()
	states = Vector{Vector{Int64}}()
	for s in allStates
		min_s = min_state(s)
		multiStates[min_s] = get(multiStates, min_s, 0) + 1
		if !(min_s in states)
			push!(states, min_s)
		end
		if verbose @show multiStates end
	end
	prob = factorial(balls) / (bowls ^ balls)
	if verbose
		@show states
		@show prob
	end
	init_probs = Float64[]
	for x in states
		if verbose print(x) end
		p = multiStates[x] * prob
		for y in x
			p *= 1 / factorial(y)
		end
		if verbose print(" -> p: $p") end
		push!(init_probs, p)
		if verbose println() end
	end
	return states, init_probs
end
states, probs = get_states(5,2,verbose=true);
````

````julia:ex8
# check if a state is absorbing
function is_absorbing(state::Vector)
	return sum(state .== 0) == length(state)-1
end

states, probs = get_states(4,2)
for st in states
	println(st, " -> is_absorbing? ", is_absorbing(st))
end
````

Ecco quindi un esempietto riassuntivo finale:

````julia:ex9
begin
	bowls = 4
	balls = 3
	println("Working with $bowls bowls and $balls balls.")
	println("All states and minimal representations:")
	for v in get_all_states(bowls,balls)
		println(v," => ", min_state(v))
	end
	println("Initial probabilities:")
	states, init_probs = get_states(bowls, balls,verbose=false)
	for i in 1:length(states)
		println(states[i], " => ", init_probs[i])
	end
end
````

### Assembliamo la soluzione
Ora che abbiamo introdotto alcuni degli elementi fondamentali per modellare il problema possiamo proseguire verso il sistema lineare che ci condurrà effettivamente alla soluzione. Dopo aver definito alcune variabili di supporto e di contesto,

````julia:ex10
nbowls = 4; nballs = 5
# definiamo gli stati e le probabilità iniziali
states, p₀ = get_states(nbowls, nballs)
# salviamo l'indice di ogni stato, per accederci con 1, 2, ecc nella matrice P
states_dict = Dict(state => i for (i, state) in enumerate(states))
nstates = length(states);
````

occorre costruire la matrice $P$. Per farlo, l'idea è che possiamo iterare sugli stati e guardare in quali altri stati possiamo arrivare seguendo i possibili modi in cui le palline possono spostarsi.

````julia:ex11
P = zeros(nstates, nstates)
for st in states
	if !is_absorbing(st)
		for i in 1:nbowls
			if st[i] > 0
				# where can the current ball move?

				# can move with direction -1
				st_new_m1 = copy(st)
				st_new_m1[i] -= 1; st_new_m1[mod1(i-1,nbowls)] +=1 # move by -1
				st_new_m1 = min_state(st_new_m1) # retrieve minimal configuration
				# or with direction 1
				st_new_p1 = copy(st)
				st_new_p1[i] -= 1; st_new_p1[mod1(i+1,nbowls)] +=1 # move by +1
				st_new_p1 = min_state(st_new_p1) # retrieve minimal configuration

				# update P matrix entries
				P[states_dict[st],states_dict[st_new_m1]] += st[i]
				P[states_dict[st],states_dict[st_new_p1]] += st[i]
			end
		end
	else
		P[states_dict[st],states_dict[st]] = 1
	end
end
# normalize the matrix by dividing each value by the sum of its row values
Int.(P)
````

````julia:ex12
for i in 1:size(P)[1]
	P[i,:] = P[i,:]/sum(P[i,:])
end
P
````

Diventa anche molto bello plottare il grafo che mostra le possibili transizioni tra gli stati della catena di Markov, dove giustamente osserviamo che esiste un solo stato assorbente (l'unico ad avere come unica freccia una freccia verso sé stesso).

````julia:ex13
# https://docs.juliaplots.org/stable/generated/graph_attributes/#graph_attributes
graphplot(P,
	names=join.(states),
	nodesize = 0.2,
	curvature_scalar=0.1,
	node_shape=:circle,
	# edge_label = P,
	# edgelabel_offset = 0,
	fontsize = 4,
	self_edge_size = 0.12,
	method=:circular,
	axis_buffer=0.2)
savefig(joinpath(@OUTPUT, "mc_graph_.svg")); # hide
````

\fig{mc_graph_.svg}

Una volta creata $P$, dobbiamo capire come usarla per risolvere il problema. Possiamo intanto osservare la struttura di $P$ (o in generale di una qualunque matrice di transizione per una catena di Markov che presenta stati assorbenti). La struttura di $P$ è infatti $$ P =\begin{pmatrix} I & O\\ R &  Q\end{pmatrix}$$
dove
- $I$ è la matrice identità, corrispondente alle transizioni degli stati assorbenti verso sé stessi
- $O$ è una matrice piena di zeri, perché dagli stati assorbenti non ci si può spostare a quelli transienti
- $Q$ è la matrice che descrive come da uno stato transiente ci si può spostare in un altro transiente
- $R$ è la matrice che descrive come da uno stato transiente ci si può spostare in uno assorbente
Noto solo ora che non ho mai chiarito cosa si intende precisamente con stati assorbenti o transienti. Forse perché supponevo che dal nome fosse tutto sommato intuibile, ma in effetti un po' di teoria anche qui non guasta. Parliamo di stati ricorrenti quando la catena di Markov ci passerà di sicuro un numero infinito di volte, da cui il nome ricorrenti: continuiamo a visitarli, ne siamo proprio affezionati. Uno stato è invece transiente se non è ricorrente, cioè se quindi,  nel lungo termine, smetteremo di visitarlo. Uno stato assorbente è la massima espressione di uno stato ricorrente in quanto, una volta entrati in uno stato assorbente, non possiamo più abbandonarlo, ne veniamo infatti "assorbiti".

Tornando alla struttura di $P$, con la logica del nostro codice esisterà un solo stato assorbente, e si troverà sempre in posizione 1. Di conseguenza, gli indici `2:end` definiranno la matrice $Q$.

Questa matrice $Q$ serve, e siamo quasi arrivati alla fine, per risolvere il sistema dei "mean hitting time". L'obiettivo era infatti di trovare $k_i^R=\mathbb{E}_i(H_R)$, ovvero il tempo (o equivalentemente, il numero di passi) medio necessario per entrare nella classe $R$ degli stati ricorrenti, partendo da un qualunque stato $i$. Nel nostro caso, in realtà, la classe $R$ è formata dall'unico stato assorbente, e quindi ricorrente, in cui le palline sono tutte riunite assieme nella stessa ciotola (in simboli, la configurazione $00\ldots 00m$). Si può ricavare che
$$ \begin{cases} k_i^R = 0 & i\in R\\ k_i^R = 1+\sum_{j\notin R}p_{ij}k_j^R & i\notin R \end{cases} $$

Trascurando, in generale, gli stati ricorrenti (nel nostro caso, di nuovo, sarebbe trascurare un solo stato) che non danno contributo, essendo che per loro $k_i^R=0$, possiamo considerare la sola seconda equazione e vederla in forma vettoriale ${\bf{k}} = {\bf{1}} + Q\bf{k}$, da cui si ricava il sistema $(I-Q)\bf{k} = \bf{1}$, che possiamo risolvere in julia in modo classico con l'operatore backslash `\ `, altrimenti usando metodi più sofisticati dal pacchetto `IterativeSolvers`.

````julia:ex14
# https://en.wikipedia.org/wiki/Absorbing_Markov_chain
Q = P[2:end,2:end]
if size(Q)[1] > 1 # c'è davvero un sistema da risolvere
	k = (I(nstates-1)-Q)\ones(nstates-1) # metodo classico
	# k = bicgstabl(I(nstates-1)-Q, ones(nstates-1)) # biconjugate gradient method
else
	k = [1/(I(nstates-1)-Q)[1]]
end
````

Una volta trovato $\bf{k}$ possiamo combinarlo con il vettore $\bf{p_0}$ delle probabilità iniziali per trovare la soluzione finale. Data la formulazione del problema, infatti, possiamo dire che
$$\mathbb{E}(\text{#passi per entrare in $R$}) = \sum_{i\in S} k_i^R \cdot \mathbb{P}(\text{partire da $i$})={\bf{k}}^T{\bf{p_0}}$$ da cui ricaviamo

````julia:ex15
slz = dot(k,p₀[2:end])
````

che corrisponde alla soluzione esatta 6875/24:

````julia:ex16
6875/24
````

### Tutto insieme
Infine, riuniamo qui all'interno di un'unica funzione ordinata tutti i passaggi finora esposti. Possiamo anche effettuare qualche piccola ottimizzazione, per esempio definendo la matrice $P$ come sparsa, grazie alle strutture importate con `SparseArrays`, in modo da potenzialmente diminuire il consumo di memoria, dato che in effetti la matrice $P$ di transizione sarà spesso vuota, essendo che non tutti gli stati conducono a tutti gli stati.

````julia:ex17
function F(nbowls, nballs; verbose=false, plot_graph=false)
	if verbose println("Deriving states and initial probabilities") end
	states, p₀ = get_states(nbowls, nballs)
	states_dict = Dict(state => i for (i, state) in enumerate(states))
	nstates = length(states)
	P = spzeros(nstates, nstates)
	if verbose println("Assembling transition matrix") end
	for st in states
		if !is_absorbing(st)
			for i in 1:nbowls
				if st[i] > 0
					# where can the current ball move?
					# can move with direction -1
					st_new_m1 = copy(st)
					st_new_m1[i] -= 1; st_new_m1[mod1(i-1,nbowls)] +=1 # move by -1
					st_new_m1 = min_state(st_new_m1) # retrieve minimal configuration
					# or with direction 1
					st_new_p1 = copy(st)
					st_new_p1[i] -= 1; st_new_p1[mod1(i+1,nbowls)] +=1 # move by +1
					st_new_p1 = min_state(st_new_p1) # retrieve minimal configuration
					# update P matrix entries
					P[states_dict[st],states_dict[st_new_m1]] += st[i]
					P[states_dict[st],states_dict[st_new_p1]] += st[i]
				end
			end
		else
			P[states_dict[st],states_dict[st]] = 1
		end
	end
	# normalize the matrix by dividing each value by the sum of its row values
	for i in 1:size(P)[1]
		P[i,:] = P[i,:]/sum(P[i,:])
	end
	if verbose
		println("Let's have a view of the sparsity of P")
		# with pure text output:
		# show(stdout,"text/plain",P)
		# SparseArrays._show_with_braille_patterns(stdout, P)
		# with a real plot:
		spy(P)
		savefig(joinpath(@OUTPUT, "P_sparsity.svg")); # hide
	end
	if plot_graph
		println("Plotting MC graph")
		graphplot(P,
		names=join.(states),
		nodesize = 0.2,
		curvature_scalar=0.1,
		node_shape=:circle,
		# edge_label = P,
		# edgelabel_offset = 0,
		fontsize = 4,
		self_edge_size = 0.12,
		method=:circular,
		axis_buffer=0.2)
		savefig("mc_graph_$(nbowls)_$(nballs).svg"); # hide
	end
	Q = P[2:end,2:end]
	if verbose println("Solving the linear system") end
	if size(Q)[1] > 1
		# k = bicgstabl(I(nstates-1)-Q, ones(nstates-1))
		k = (I(nstates-1)-Q)\ones(nstates-1)
	else
		k = [1/(I(nstates-1)-Q)[1]]
	end
	slz = dot(k,p₀[2:end])
	if verbose println("done!") end
	return slz
end
F(12,6,verbose=true,plot_graph=false)
````

\fig{P_sparsity.svg}

````julia:ex18
function G(N,M)
	S = 0
	for n in 2:N, m in 2:M
		println("(n,m)=($n,$m)")
		@time S += F(n,m)
		println("")
	end
	println("done!")
	return S
end

# G(12,12) risolve il problema
G(4,4); # giusto un esempio
````

````julia:ex19
@time F(10,10) # altro esempio di quanto ci mette il codice con n ed m altini
````

